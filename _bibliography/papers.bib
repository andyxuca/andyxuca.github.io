---
---
@article{SoundForVR,
  abbr={soundForVR},
  title={Implementing an Audio Processing System to Simulate Realistic Distance with Sound},
  author={Xu, Andy and Hariman, Christopher and Yang, Ann, and Inamti, Ankita and Jaroscewicz, Martin},
  abstract={In order to have a more immersive auditory virtual reality experience, a method to better reproduce a sense of distance is required. We recorded sounds at varying distances to observe changes in the intensity and spectral content of moving sounds. By obtaining data from spectrograms, applying different filters, and implementing sound synthesis techniques in the time domain, we created a system that virtually emulates how we perceive those changes. We were able to simulate a sense of distance with synthesized sounds.},
  journal={Vanderbilt Young Scientist},
  year={2022},
  month={May},
  url={http://youngscientistjournal.org/youngscientistjournal/article/implementing-an-audio-processing-system-to-simulate-realistic-distance-with-sound},
  pdf={xu_article_revised.pdf},
  preview={spectrogram.png}
}

@article{SmartAvatar,
  abbr={smartAvatar},
  title={SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents},
  author={Huang-Menders, Alexander and Liu, Xinhang, and Xu, Andy and Zhang, Yuyao, and Tang, Chi-Keung, and Tai, Yu-Wing},
  abstract={SmartAvatar is a vision-language-agent-driven framework for generating fully rigged, animation-ready 3D human avatars from a single photo or textual prompt. While diffusion-based methods have made progress in general 3D object generation, they continue to struggle with precise control over human identity, body shape, and animation readiness. In contrast, SmartAvatar leverages the commonsense reasoning capabilities of large vision-language models (VLMs) in combination with off-the-shelf parametric human generators to deliver high-quality, customizable avatars. A key innovation is an autonomous verification loop, where the agent renders draft avatars, evaluates facial similarity, anatomical plausibility, and prompt alignment, and iteratively adjusts generation parameters for convergence. This interactive, AI-guided refinement process promotes fine-grained control over both facial and body features, enabling users to iteratively refine their avatars via natural-language conversations. Unlike diffusion models that rely on static pre-trained datasets and offer limited flexibility, SmartAvatar brings users into the modeling loop and ensures continuous improvement through an LLM-driven procedural generation and verification system. The generated avatars are fully rigged and support pose manipulation with consistent identity and appearance, making them suitable for downstream animation and interactive applications. Quantitative benchmarks and user studies demonstrate that SmartAvatar outperforms recent text- and image-driven avatar generation systems in terms of reconstructed mesh quality, identity fidelity, attribute accuracy, and animation readiness, making it a versatile tool for realistic, customizable avatar creation on consumer-grade hardware.},
  journal={arXiv},
  year={2025},
  month={June},
  url={https://arxiv.org/abs/2506.04606},
  pdf={xu_article_revised.pdf},
  preview={spectrogram.png}
}
